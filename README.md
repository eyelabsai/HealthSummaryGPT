# WhisperTranscription

A minimal Node.js service that records audio via the browser, transcribes speech using OpenAIâ€™s `gpt-4o-mini-transcribe`, thenâ€”on demandâ€”summarises the transcript with `gpt-4o-mini`.  

## Overview  
This application provides:  
1. A simple front end (HTML/JavaScript) that captures microphone input and uploads it as a WebM blob.  
2. An Express-powered back end that:  
   - Converts the browser-generated WebM to WAV (via `ffmpeg`).  
   - Streams the resulting WAV to OpenAIâ€™s transcription endpoint.  
   - Returns the transcript to the browser.  
   - Accepts a transcript and returns a concise summary via the LLM.  

You can record your voice, obtain a transcription, then click â€œSummariseâ€ to receive a condensed version.  

## Prerequisites  
- **Node.js** (v16 or later) and **npm** installed.  
- **ffmpeg** installed on your system (Homebrew, apt, etc.), so that the WebMâ†’WAV conversion works.  
- An **OpenAI API key** with access to the `gpt-4o-mini-transcribe` and `gpt-4o-mini` models.  

## Installation  

1. **Clone the repository** (or copy files) into a local folder:  
   ```bash
   git clone https://github.com/your-username/WhisperTranscription.git
   cd WhisperTranscription
   ```

	2.	Install Node.js dependencies:
 3.	
```
npm install
```

	3.	Install ffmpeg (if not already):
	â€¢	macOS (Homebrew):
```
brew install ffmpeg
```

	â€¢	Ubuntu/Debian:
```
sudo apt update
sudo apt install ffmpeg
```

	â€¢	Confirm installation with:
```
ffmpeg -version
```

	4.	Configure environment variables:
Create a file named .env in the project root with:

```
OPENAI_API_KEY=sk-YOUR_OPENAI_KEY_HERE
```

Ensure that the key is valid and grants access to both gpt-4o-mini-transcribe and gpt-4o-mini.

Project Structure

.
â”œâ”€â”€ public
â”‚   â””â”€â”€ index.html           â† Front-end HTML/JS for recording, transcription, summarisation
â”œâ”€â”€ uploads                  â† Multerâ€™s temporary storage for incoming audio blobs
â”œâ”€â”€ server.js                â† Main Express server
â”œâ”€â”€ package.json
â””â”€â”€ .env                     â† Contains your OpenAI API key (not committed to Git)

Configuration
	â€¢	package.json
	â€¢	Has "type": "module" set, so Node treats .js files as ES modules.
	â€¢	Dependencies:
	â€¢	express for HTTP server.
	â€¢	multer for handling file uploads.
	â€¢	openai v5 SDK to call OpenAIâ€™s endpoints.
	â€¢	dotenv to load environment variables.
	â€¢	server.js
	â€¢	Imports createReadStream and renameSync from fs, plus execSync from child_process.
	â€¢	Defines two endpoints:
	1.	POST /transcribe: accepts an audio file (WebM blob).
	â€¢	Renames the file to .webm.
	â€¢	Converts WebM â†’ WAV via ffmpeg.
	â€¢	Streams the WAV to OpenAIâ€™s transcription API (gpt-4o-mini-transcribe).
	â€¢	Returns { success: true, transcript } on success.
	2.	POST /summarise: expects JSON { transcript: "â€¦" }.
	â€¢	Calls OpenAIâ€™s chat completion (gpt-4o-mini) with a summarisation prompt.
	â€¢	Returns { success: true, summary }.
	â€¢	public/index.html
	â€¢	Simple UI with:
	â€¢	â€œStart Recordingâ€ and â€œStop Recordingâ€ buttons (MediaRecorder API).
	â€¢	A <textarea> to show the transcript.
	â€¢	A â€œSummariseâ€ button to request a summary.
	â€¢	Status messages to indicate progress.

Usage
	1.	Start the server:

node server.js

You should see:

Server listening on port 3000


	2.	Open your browser at

http://localhost:3000

â€“ You will see a page with â€œStart Recordingâ€ / â€œStop Recordingâ€ buttons.

	3.	Record & transcribe:
	â€¢	Click â€œğŸ¤ Start Recordingâ€, speak into your microphone, then click â€œâ¹ Stop Recordingâ€.
	â€¢	The page will upload the WebM blob, server converts â†’ WAV â†’ transcribes with gpt-4o-mini-transcribe.
	â€¢	Once complete, the raw transcript appears in the first textarea.
	4.	Summarize:
	â€¢	Click â€œğŸ“„ Summarize Transcriptâ€.
	â€¢	The transcript is sent to the server, which calls gpt-4o-mini to produce a concise summary.
	â€¢	The summary is displayed in the second textarea.

Endpoints

POST /transcribe
	â€¢	Content-Type: multipart/form-data
	â€¢	Form Field:
	â€¢	audio (file): Browser-recorded audio blob (WebM).

Response (JSON):

{ 
  "success": true, 
  "transcript": "â€¦your transcribed textâ€¦" 
}

Or, on failure:

{ 
  "success": false, 
  "error": "Transcription failed." 
}

POST /summarise
	â€¢	Content-Type: application/json
	â€¢	JSON Body:

{ 
  "transcript": "â€¦full transcript textâ€¦" 
}

Response (JSON):

{ 
  "success": true, 
  "summary": "â€¦concise summaryâ€¦" 
}

Or, on failure:

{ 
  "success": false, 
  "error": "Summarisation failed." 
}

Troubleshooting
	1.	â€œffmpeg: command not foundâ€
	â€¢	You must install ffmpeg (via Homebrew on macOS or your package manager).
	â€¢	Confirm with ffmpeg -version.
	2.	â€œAudio file might be corrupted or unsupportedâ€
	â€¢	Ensure the front end is indeed sending a WebM blob (the code appends .webm).
	â€¢	Check that ffmpeg successfully produces a valid WAV file (inspect uploads/xyz.wav).
	3.	â€œCould not parse multipart formâ€
	â€¢	This occurs if you send a Buffer instead of a proper ReadStream. Make sure the code uses createReadStream on a renamed file with a correct extension.
	4.	Missing API key or invalid model
	â€¢	Verify your .env has a valid OPENAI_API_KEY.
	â€¢	Confirm access to gpt-4o-mini-transcribe and gpt-4o-mini on your OpenAI plan.

Licence

MIT Licence
